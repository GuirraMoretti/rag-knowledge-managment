{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9eaa37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, logging, itertools\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "# Fallback: from langchain_experimental.open_clip import OpenCLIPEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea58e30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_DIR           = \"/home/gui/rag-knowledge-managment/docs/pdfs\"        # folder with many PDFs\n",
    "PERSIST_DIR       = \"./chroma_db\"   # where Chroma stores the collection\n",
    "COLLECTION_NAME   = \"my-rag\"\n",
    "CHUNK_SIZE        = 1000\n",
    "CHUNK_OVERLAP     = 150\n",
    "TOP_K             = 3              # how many chunks to retrieve\n",
    "LLM_MODEL_NAME    = \"llama-3.3-70b-versatile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd4cadcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vector_store():\n",
    "    \"\"\"Load PDFs → split → embed → store (runs once or when you add new PDFs).\"\"\"\n",
    "    loader = PyPDFDirectoryLoader(PDF_DIR, recursive=True)\n",
    "    docs = loader.load()\n",
    "    logging.info(\"Loaded %d pages from %s\", len(docs), PDF_DIR)\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP\n",
    "    )\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    logging.info(\"Split into %d chunks\", len(chunks))\n",
    "\n",
    "    embeddings = MistralAIEmbeddings()  # relies on $MISTRAL_API_KEY\n",
    "    # embeddings = OpenCLIPEmbeddings(model_name=\"ViT-H-14\")  # fallback\n",
    "\n",
    "    vectordb = Chroma.from_documents(\n",
    "        chunks,\n",
    "        embedding=embeddings,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        persist_directory=PERSIST_DIR,\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    logging.info(\"Vector DB persisted at %s\", PERSIST_DIR)\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba44dc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vector_store():\n",
    "    \"\"\"Load an existing Chroma collection without re-embedding.\"\"\"\n",
    "    embeddings = MistralAIEmbeddings()\n",
    "    return Chroma(\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        persist_directory=PERSIST_DIR,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c603485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_chain_with_validation(vectordb):\n",
    "    retriever = vectordb.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "\n",
    "    # 1) Prompt original para gerar rascunho\n",
    "    draft_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Você é um assistente útil. Use somente o contexto abaixo.\n",
    "=== Contexto ===\n",
    "{context}\n",
    "\n",
    "=== Pergunta ===\n",
    "{question}\n",
    "\n",
    "=== Resposta ===\n",
    "\"\"\"\n",
    "    )\n",
    "    llm = ChatGroq(model_name=LLM_MODEL_NAME)\n",
    "\n",
    "    # 2) Prompt de validação\n",
    "    validate_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Eu vou te passar o contexto e uma resposta gerada.\n",
    "Verifique se a resposta está:\n",
    "  a) correta segundo o contexto,\n",
    "  b) não adiciona informação não suportada,\n",
    "  c) aborda a pergunta de forma completa.\n",
    "\n",
    "Se estiver OK, retorne exatamente a resposta validada.\n",
    "Caso contrário, retorne “Não consegui validar a resposta com o contexto.”.\n",
    "\n",
    "=== Contexto ===\n",
    "{context}\n",
    "\n",
    "=== Resposta Gerada ===\n",
    "{draft}\n",
    "\"\"\"\n",
    "    )\n",
    "    validator = ChatGroq(model_name=LLM_MODEL_NAME)\n",
    "\n",
    "    # 3) Pipeline LCEL\n",
    "    rag_with_val = (\n",
    "        {\n",
    "            \"question\": RunnablePassthrough(),\n",
    "            # gera um único string com todos os chunks\n",
    "            \"context\": retriever | (lambda docs: \"\\n\\n\".join(d.page_content for d in docs))\n",
    "        }\n",
    "        # 3.1) Gera o rascunho\n",
    "        | draft_prompt\n",
    "        | llm\n",
    "\n",
    "        # 3.2) Valida o rascunho\n",
    "        | (  # empacota para validar\n",
    "            {\n",
    "                \"context\": RunnablePassthrough(),  # reutiliza o mesmo contexto\n",
    "                \"draft\": RunnablePassthrough()     # pega a saída do llm\n",
    "            }\n",
    "            | validate_prompt\n",
    "            | validator\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return rag_with_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae22d6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gui/.local/share/virtualenvs/rag-knowledge-managment-oo58KjW1/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:181: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pergunte algo (vazio para sair):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 19:33:40,040 [INFO] HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-22 19:33:42,305 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-22 19:33:43,128 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Não consegui validar a resposta com o contexto.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 1119, 'total_tokens': 1130, 'completion_time': 0.04, 'prompt_time': 0.085143679, 'queue_time': 0.27589426100000003, 'total_time': 0.125143679}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_2ddfbb0da0', 'finish_reason': 'stop', 'logprobs': None} id='run--850c00b7-262e-4cbd-a565-a0be587b5ca4-0' usage_metadata={'input_tokens': 1119, 'output_tokens': 11, 'total_tokens': 1130}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 19:34:21,417 [INFO] HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-22 19:34:22,739 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-22 19:34:23,803 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='O objetivo geral da pesquisa de gestão do conhecimento em startups é entender como as startups podem gerenciar, descobrir, mapear, classificar, captar, distribuir, criar, multiplicar e reter conhecimento com eficiência, eficácia e efetividade, considerando os desafios únicos que elas enfrentam, como a escassez de recursos e a falta de processos estruturados, e identificar práticas eficazes de gestão do conhecimento que possam proporcionar um diferencial competitivo significativo para essas empresas.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 128, 'prompt_tokens': 747, 'total_tokens': 875, 'completion_time': 0.465454545, 'prompt_time': 0.055105503, 'queue_time': 0.278438983, 'total_time': 0.520560048}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'finish_reason': 'stop', 'logprobs': None} id='run--f24b4b5d-2536-4b7e-819f-19f79bbdacdc-0' usage_metadata={'input_tokens': 747, 'output_tokens': 128, 'total_tokens': 875}\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    ")\n",
    "\n",
    "# build once, then comment out and just 'load_vector_store'\n",
    "#vectordb = build_vector_store()  # first run\n",
    "vectordb = load_vector_store()  # subsequent runs\n",
    "\n",
    "rag_chain = build_rag_chain_with_validation(vectordb)\n",
    "\n",
    "# Simple REPL\n",
    "print(\"Pergunte algo (vazio para sair):\")\n",
    "while (q := input(\"> \").strip()):\n",
    "    resposta_final = rag_chain.invoke(q)\n",
    "    print(resposta_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-knowledge-managment-oo58KjW1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
